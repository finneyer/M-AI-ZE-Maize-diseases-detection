{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d11444d-4a37-4242-a2dd-64a83c721bf8",
   "metadata": {},
   "source": [
    "# YOLOv11 Model Training and Evaluation for Maize Disease Detection\n",
    "\n",
    "This notebook provides a complete pipeline for training and evaluating YOLOv11 object detection models on the **maize lesion dataset**, structured across multiple dataset splits: `SID01`, `SID02`, and `SID03`.\n",
    "\n",
    "Key features of this pipeline include:\n",
    "\n",
    "- **Automated training** across different YOLO variants (`n`, `m`, `l`), batch sizes, and splits.\n",
    "- Support for:\n",
    "  - `SID02` subtype-specific training (`boom`, `drone`, `handheld`)\n",
    "  - `SID03` K-Fold cross-validation.\n",
    "- **Custom evaluation using the RAAD metric** (Relative Affected Area Difference), assessing bounding box area agreement between predictions and ground truth.\n",
    "- **Integration with Weights & Biases (wandb)** for:\n",
    "  - Tracking training runs and hyperparameters.\n",
    "  - Logging test metrics and qualitative predictions.\n",
    "  - Uploading per-image evaluation tables and **collage visualizations** for best/worst cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd99517-1d1a-42f8-acd9-8b7f78c2cd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.3.151-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (3.10.1)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (1.15.2)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (2.2.3)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (4.25.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.11.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/conda/lib/python3.12/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Collecting filelock (from torch>=1.8.0->ultralytics)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (78.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Using cached ultralytics-8.3.151-py3-none-any.whl (1.0 MB)\n",
      "Using cached wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.2 MB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "Using cached sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)\n",
      "Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "Using cached ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Using cached setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, triton, setproctitle, sentry-sdk, opencv-python, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, wandb, nvidia-cusolver-cu12, torch, ultralytics-thop, torchvision, ultralytics\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.3.101\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.3.101:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.3.101\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.4.107\n",
      "    Uninstalling nvidia-curand-cu12-10.3.4.107:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.4.107\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.3.101\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.3.101:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.3.101\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.3.107\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.3.107:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.3.107\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.3.101\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.3.101:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.3.101\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.3.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.3.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.3.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.2.0.103\n",
      "    Uninstalling nvidia-cusparse-cu12-12.2.0.103:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.2.0.103\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.12.1\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.12.1:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.12.1\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.7.29\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.7.29:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.7.29\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.5.4.101\n",
      "    Uninstalling nvidia-cusolver-cu12-11.5.4.101:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.5.4.101\n",
      "Successfully installed filelock-3.18.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opencv-python-4.11.0.86 sentry-sdk-2.29.1 setproctitle-1.3.6 torch-2.7.1 torchvision-0.22.1 triton-3.3.1 ultralytics-8.3.151 ultralytics-thop-2.0.14 wandb-0.20.1\n",
      "Collecting dotenv\n",
      "  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n",
      "Collecting shapely\n",
      "  Using cached shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.12/site-packages (from shapely) (1.26.4)\n",
      "Using cached shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: shapely\n",
      "Successfully installed shapely-2.1.1\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (4.25.6)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.12/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/conda/lib/python3.12/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ultralytics wandb\n",
    "!pip install dotenv\n",
    "!pip install shapely\n",
    "!pip install wandb opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab02345-a3f0-424f-9630-40700ac65d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import wandb\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm\n",
    "from shapely.geometry import box, MultiPolygon\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Tuple\n",
    "import traceback\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a3cd17-1d51-4292-bc75-36d17d218d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDict(\"/home/jovyan/.config/Ultralytics/settings.json\"):\n",
      "{\n",
      "  \"settings_version\": \"0.0.6\",\n",
      "  \"datasets_dir\": \"/home/jovyan/DSPRO2/M-AI-ZE-Maize-diseases-detection/notebooks/datasets\",\n",
      "  \"weights_dir\": \"weights\",\n",
      "  \"runs_dir\": \"runs\",\n",
      "  \"uuid\": \"8a115bbf5049f0fe55cf2ccd8be54ca8bfded6b963fd272724a959bb525556d2\",\n",
      "  \"sync\": true,\n",
      "  \"api_key\": \"\",\n",
      "  \"openai_api_key\": \"\",\n",
      "  \"clearml\": true,\n",
      "  \"comet\": true,\n",
      "  \"dvc\": true,\n",
      "  \"hub\": true,\n",
      "  \"mlflow\": true,\n",
      "  \"neptune\": true,\n",
      "  \"raytune\": true,\n",
      "  \"tensorboard\": false,\n",
      "  \"wandb\": true,\n",
      "  \"vscode_msg\": true,\n",
      "  \"openvino_msg\": true\n",
      "}\n",
      "ðŸ’¡ Learn more about Ultralytics Settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n"
     ]
    }
   ],
   "source": [
    "!yolo settings wandb=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fefa102-b160-42c1-b14a-474ce0149f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_API_KEY: [69ca...]\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "print(f\"WANDB_API_KEY: [{wandb_api_key[:4]}...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7945424-eadb-4480-af0c-37eb29d3c99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrueedi-tobias\u001b[0m (\u001b[33mrueedi-tobias-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89f480-b2c4-43fa-90a5-09446cfd786e",
   "metadata": {},
   "source": [
    "## Get Data and set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953504ea-b0d4-483a-927f-1472d9511ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = [8, 16, 32]\n",
    "DEFAULT_EPOCHS = 40\n",
    "DEFAULT_KFOLDS = 5\n",
    "BASE_PATH = Path(\"/exchange/dspro2/M-AI-ZE/data/adjusted/1.2/splits\")\n",
    "MODEL_WEIGHTS = {\"s\": \"yolo11s.pt\", \"n\": \"yolo11n.pt\", \"m\": \"yolo11m.pt\", \"l\": \"yolo11l.pt\"}\n",
    "SUBTYPES = [\"boom\", \"drone\", \"handheld\"]\n",
    "IMG_SIZE  = 640\n",
    "TRAIN_PROJECT_PREFIX = \"V1_2-maize_disease_detection_train\"\n",
    "EVAL_PROJECT_PREFIX  = \"V1_2-maize_disease_detection_eval\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35ba30-d194-4078-9cda-185508c53ca3",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915ebfe1-f1dd-4c16-8c9f-c0331803b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_coverage_score(pred_boxes, gt_boxes):\n",
    "    \"\"\"\n",
    "    Returns a score from 0 to 100 based on how well the predicted boxes cover the ground truth boxes.\n",
    "    Full coverage (prediction area == label area) gives 100.\n",
    "    Over-prediction (prediction area > label area) is penalized.\n",
    "    Under-prediction (prediction area < label area) is also penalized.\n",
    "    Args:\n",
    "        pred_boxes: list or np.array of [ymin, xmin, ymax, xmax] (normalized 0-1)\n",
    "        gt_boxes: list or np.array of [ymin, xmin, ymax, xmax] (normalized 0-1)\n",
    "    Returns:\n",
    "        float: score between 0 and 100\n",
    "    \"\"\"\n",
    "    def total_area(boxes):\n",
    "        if len(boxes) == 0:\n",
    "            return 0.0\n",
    "        boxes = np.array(boxes)\n",
    "        heights = np.clip(boxes[:, 2] - boxes[:, 0], 0, 1)\n",
    "        widths = np.clip(boxes[:, 3] - boxes[:, 1], 0, 1)\n",
    "        return np.sum(heights * widths)\n",
    " \n",
    "    area_pred = total_area(pred_boxes)\n",
    "    area_gt = total_area(gt_boxes)\n",
    " \n",
    "    if area_gt == 0:\n",
    "        return 0.0 if area_pred > 0 else 100.0\n",
    " \n",
    "    # Score is 100 if areas match, penalized for over/under\n",
    "    ratio = area_pred / area_gt\n",
    "    if ratio <= 1:\n",
    "        score = ratio * 100\n",
    "    else:\n",
    "        # Over-prediction: penalize more as prediction exceeds label\n",
    "        score = max(0, 100 - (ratio - 1) * 100)\n",
    "    return score\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "\n",
    "    inter_w = max(0, inter_x_max - inter_x_min)\n",
    "    inter_h = max(0, inter_y_max - inter_y_min)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union_area = area1 + area2 - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "def mean_precision_at_iou(pred_boxes_list, gt_boxes_list, iou_threshold=0.5):\n",
    "    aps = []\n",
    "    for pred_boxes, gt_boxes in zip(pred_boxes_list, gt_boxes_list):\n",
    "        ap = average_precision(pred_boxes, gt_boxes, iou_threshold)\n",
    "        aps.append(ap)\n",
    "    return np.mean(aps)\n",
    "\n",
    "def average_precision(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
    "    if len(pred_boxes) == 0:\n",
    "        return 0.0\n",
    "    matched = set()\n",
    "    tp = 0\n",
    "    for pb in pred_boxes:\n",
    "        for i, gb in enumerate(gt_boxes):\n",
    "            if i in matched:\n",
    "                continue\n",
    "            if compute_iou(pb, gb) >= iou_threshold:\n",
    "                tp += 1\n",
    "                matched.add(i)\n",
    "                break\n",
    "    fp = len(pred_boxes) - tp\n",
    "    fn = len(gt_boxes) - tp\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return precision \n",
    "    \n",
    "\n",
    "def calculate_raad(pred_boxes, true_boxes, img_w=IMG_SIZE, img_h=IMG_SIZE,\n",
    "                   epsilon=1e-6, normalize=True):\n",
    "    if not pred_boxes and not true_boxes:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    if not pred_boxes:\n",
    "        true_area = sum((b[2]-b[0])*(b[3]-b[1]) for b in true_boxes)\n",
    "        return 1.0, 0.0, true_area\n",
    "    if not true_boxes:\n",
    "        pred_area = sum((b[2]-b[0])*(b[3]-b[1]) for b in pred_boxes)\n",
    "        return 1.0, pred_area, 0.0\n",
    "\n",
    "    if normalize:\n",
    "        pred_boxes = [[b[0]*img_w, b[1]*img_h, b[2]*img_w, b[3]*img_h] for b in pred_boxes]\n",
    "        true_boxes = [[b[0]*img_w, b[1]*img_h, b[2]*img_w, b[3]*img_h] for b in true_boxes]\n",
    "\n",
    "    pred_poly = MultiPolygon([box(*b) for b in pred_boxes]).buffer(0)\n",
    "    true_poly = MultiPolygon([box(*b) for b in true_boxes]).buffer(0)\n",
    "\n",
    "    pred_area = pred_poly.area\n",
    "    true_area = true_poly.area\n",
    "    raad = abs(pred_area - true_area) / max(true_area, epsilon)\n",
    "    return raad, pred_area, true_area\n",
    "\n",
    "\n",
    "def load_bbox_csv(csv_path: Path) -> Dict[str, List[Tuple[int,int,int,int]]]:\n",
    "    df = pd.read_csv(csv_path, header=None, skiprows=1)\n",
    "    out = {}\n",
    "    for _, row in df.iterrows():\n",
    "        out.setdefault(row[0], []).append(tuple(map(int, row[1:5])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def suppress_yolo_logging():\n",
    "    \"\"\"Suppress the YOLO logging temporarily.\"\"\"\n",
    "    logger = logging.getLogger(\"ultralytics\")\n",
    "    original_level = logger.level\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    return logger, original_level\n",
    "\n",
    "def restore_yolo_logging(logger, original_level):\n",
    "    \"\"\"Restore the YOLO logging to its original level.\"\"\"\n",
    "    logger.setLevel(original_level)\n",
    "\n",
    "def _make_sid02_subset(root: Path, subtype: str) -> Path:\n",
    "    \"\"\"\n",
    "    Creates subfolder for split SID02, because Yolo cant use subfolder.\n",
    "    Example ../train/boom need to be ../train \n",
    "    Therefore a temporary Folder gets created.\n",
    "    \"\"\"\n",
    "    tmp = root.parent / f\"{root.name}_{subtype}\"\n",
    "    if tmp.exists():\n",
    "        return tmp\n",
    "\n",
    "    for p in (\"images\", \"labels\"):\n",
    "        for split in (\"train\", \"val\"):\n",
    "            (tmp / p / split).mkdir(parents=True, exist_ok=True)\n",
    "    (tmp / \"images\" / \"test\").symlink_to(root / \"images\" / \"test\")\n",
    "    (tmp / \"labels\" / \"test\").symlink_to(root / \"labels\" / \"test\")\n",
    "\n",
    "    for phase in (\"train\", \"val\"):\n",
    "        lbl_src_root = root / \"labels\" / phase / subtype\n",
    "        img_src_root = root / \"images\" / phase\n",
    "        for lbl in lbl_src_root.glob(\"*.txt\"):\n",
    "            shutil.copy(lbl, tmp / \"labels\" / phase / lbl.name)\n",
    "            img_src = img_src_root / f\"{lbl.stem}.jpg\"\n",
    "            if img_src.exists():\n",
    "                (tmp / \"images\" / phase / img_src.name).symlink_to(img_src)\n",
    "\n",
    "    yaml_dict = {\n",
    "        \"train\": str(tmp / \"images\" / \"train\"),\n",
    "        \"val\":   str(tmp / \"images\" / \"val\"),\n",
    "        \"test\":  str(tmp / \"images\" / \"test\"),\n",
    "        \"nc\": 1,\n",
    "        \"names\": [\"lesion\"],\n",
    "    }\n",
    "    (tmp / \"data.yaml\").write_text(yaml.safe_dump(yaml_dict))\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2838ed3-e8af-4f4d-a8ec-2815249d4fba",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045aa155-9561-40b0-9ace-ae7b75484bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model_size=\"n\",    \n",
    "        split=\"SID01\",         \n",
    "        subtype=None,           \n",
    "        fold_id=None,            \n",
    "        epochs=10,\n",
    "        batch=16,\n",
    "        lr=0.01,\n",
    "        project = \"maize_disease_detection\",\n",
    "        run_name = \"not_set\"):\n",
    "\n",
    "    split_root = BASE_PATH / split\n",
    "\n",
    "\n",
    "    if split == \"SID02\" and subtype:\n",
    "        split_root = _make_sid02_subset(split_root, subtype)\n",
    "\n",
    "    if split == \"SID03\" and fold_id is not None:\n",
    "        split_root = split_root.parent / f\"{split}_kfold\" / f\"fold{fold_id}\"\n",
    "\n",
    "    dataset_yaml = split_root / \"data.yaml\"\n",
    "    if not dataset_yaml.exists():\n",
    "        raise FileNotFoundError(f\"YAML nicht gefunden: {dataset_yaml}\")\n",
    "\n",
    "    model_file = MODEL_WEIGHTS[model_size]\n",
    "\n",
    "    try:\n",
    "        model = YOLO(model_file)\n",
    "        results = model.train(\n",
    "            data=str(dataset_yaml),\n",
    "            epochs=epochs,\n",
    "            imgsz=IMG_SIZE,\n",
    "            lr0=lr,\n",
    "            batch=batch,\n",
    "            name=run_name,\n",
    "            project=project,\n",
    "            exist_ok=True\n",
    "        )\n",
    "\n",
    "        ckpt_dir = Path(\"runs\") / \"detect\" / run_name / \"weights\"\n",
    "        weights_path = next((ckpt_dir / f).as_posix() for f in (\"best.pt\", \"last.pt\") if (ckpt_dir / f).exists())\n",
    "\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "\n",
    "        cfg = dict(model=model_file, split=split, subtype=subtype,\n",
    "                   fold_id=fold_id, epochs=epochs, batch=batch,\n",
    "                   lr=lr, weights_path=weights_path)\n",
    "        wb = wandb.init(project=project,\n",
    "                        name=run_name, config=cfg, reinit=True)\n",
    "\n",
    "        mAP50     = results.results_dict.get(\"metrics/mAP50\", 0)\n",
    "        mAP50_95  = results.results_dict.get(\"metrics/mAP50-95\", 0)\n",
    "        wb.log({\"mAP50\": mAP50, \"mAP50-95\": mAP50_95})\n",
    "\n",
    "        val_img_dir = split_root / \"images\" / \"val\"\n",
    "        for i, img_file in enumerate(list(val_img_dir.glob(\"*.jpg\"))[:5]):\n",
    "            pred = model.predict(str(img_file), conf=0.25)[0]\n",
    "            img = cv2.cvtColor(cv2.imread(str(img_file)), cv2.COLOR_BGR2RGB)\n",
    "            for b in pred.boxes:\n",
    "                x1, y1, x2, y2 = map(int, b.xyxy[0])\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            wb.log({f\"val_image_{i}\": wandb.Image(img, caption=img_file.name)})\n",
    "\n",
    "        wb.finish()\n",
    "        return weights_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Training Error:\", e)\n",
    "        traceback.print_exc()\n",
    "        return MODEL_WEIGHTS[model_size] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1ea1a-45b3-4a04-8552-b2cb4a2944f0",
   "metadata": {},
   "source": [
    "## Evaluate RAAD on Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2080b9f0-1991-41f5-9824-35436c7273c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_raad(model_path, split=\"SID01\", split_root=None, project=\"maize_disease_detection_eval\", run_name=None):\n",
    "    TARGET_SIZE = (640, 640)\n",
    "    \n",
    "    if split_root is None:\n",
    "        split_root = BASE_PATH / split\n",
    "    else:\n",
    "        split_root = Path(split_root)\n",
    "\n",
    "    test_img_dir = split_root / \"images\" / \"test\"\n",
    "    test_csv_path = split_root / \"labels\" / \"test\" / \"bboxes_test.csv\"\n",
    "\n",
    "    if not test_csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {test_csv_path}\")\n",
    "    if not test_img_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {test_img_dir}\")\n",
    "\n",
    "    bounding_boxes = load_bbox_csv(test_csv_path)\n",
    "    model_name = Path(model_path).stem\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=project,\n",
    "        name=run_name or f\"test_eval_{Path(model_path).stem}_{split_root.name}\",        \n",
    "        config={\"model_path\": str(model_path), \"split\": split_root.name},\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "\n",
    "        results = {\n",
    "            \"image\": [],\n",
    "            \"raad\": [],\n",
    "            \"pred_area\": [],\n",
    "            \"true_area\": [],\n",
    "            \"prediction_count\": [],\n",
    "            \"truth_count\": [],\n",
    "            \"area_score\" : []\n",
    "        }\n",
    "\n",
    "        pred_boxes_all = []\n",
    "        true_boxes_all = []\n",
    "\n",
    "        test_imgs = sorted([f for f in os.listdir(test_img_dir) if f.endswith(('.jpg', '.png'))])\n",
    "        for img_file in tqdm(test_imgs, desc=f\"Evaluating {split_root.name}\"):\n",
    "            img_path = test_img_dir / img_file\n",
    "\n",
    "            if img_file not in bounding_boxes:\n",
    "                print(f\"[!] Missing GT: {img_file}\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(str(img_path))\n",
    "            h, w = img.shape[:2]\n",
    "\n",
    "            logger, original_level = suppress_yolo_logging()\n",
    "            try:\n",
    "                preds = model.predict(str(img_path), save=False, imgsz=(h, w))[0]\n",
    "            finally:\n",
    "                restore_yolo_logging(logger, original_level)\n",
    "            \n",
    "            pred_boxes = preds.boxes.xyxy.cpu().numpy().tolist()\n",
    "            true_boxes = bounding_boxes[img_file]\n",
    "\n",
    "            pred_boxes_all.append(pred_boxes)\n",
    "            true_boxes_all.append(true_boxes)\n",
    "\n",
    "\n",
    "            raad, pred_area, true_area = calculate_raad(pred_boxes, true_boxes, w, h, normalize=False)\n",
    "\n",
    "        \n",
    "            norm_pred = [[b[1]/h, b[0]/w, b[3]/h, b[2]/w] for b in pred_boxes]\n",
    "            norm_true = [[b[1]/h, b[0]/w, b[3]/h, b[2]/w] for b in true_boxes]\n",
    "            \n",
    "            area_score = area_coverage_score(norm_pred, norm_true)\n",
    "\n",
    "\n",
    "            results[\"image\"].append(img_file)\n",
    "            results[\"raad\"].append(raad)\n",
    "            results[\"pred_area\"].append(pred_area)\n",
    "            results[\"true_area\"].append(true_area)\n",
    "            results[\"prediction_count\"].append(len(pred_boxes))\n",
    "            results[\"truth_count\"].append(len(true_boxes))\n",
    "            results[\"area_score\"].append(area_score)\n",
    "\n",
    "\n",
    "        approx_mAP50 = mean_precision_at_iou(pred_boxes_all, true_boxes_all)\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        avg_raad = df[\"raad\"].mean()\n",
    "        median_raad = df[\"raad\"].median()\n",
    "        max_raad = df[\"raad\"].max()\n",
    "        \n",
    "        avg_area_score = df[\"area_score\"].mean()\n",
    "        avg_pred_area = df[\"pred_area\"].mean()\n",
    "        avg_true_area = df[\"true_area\"].mean()\n",
    "        avg_pred_count = df[\"prediction_count\"].mean()\n",
    "        avg_truth_count = df[\"truth_count\"].mean()\n",
    "        avg_count_ratio = avg_pred_count / max(avg_truth_count, 1e-6)\n",
    "        \n",
    "        run.log({\n",
    "            \"avg_raad\": avg_raad,\n",
    "            \"median_raad\": median_raad,\n",
    "            \"max_raad\" : max_raad,\n",
    "            \"avg_pred_area\": avg_pred_area,\n",
    "            \"avg_true_area\": avg_true_area,\n",
    "            \"avg_pred_count\": avg_pred_count,\n",
    "            \"avg_truth_count\": avg_truth_count,\n",
    "            \"avg_count_ratio\": avg_count_ratio,\n",
    "            \"avg_area_score\": avg_area_score,\n",
    "            \"mean_precision@0.5\": approx_mAP50\n",
    "        })\n",
    "\n",
    "        table = wandb.Table(columns=[\"image\", \"raad\", \"pred_area\", \"true_area\", \n",
    "                                     \"prediction_count\", \"truth_count\", \"count_ratio\", \"area_score\"])\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            count_ratio = row[\"prediction_count\"] / max(row[\"truth_count\"], 1e-6)\n",
    "            table.add_data(row[\"image\"], row[\"raad\"], row[\"pred_area\"], row[\"true_area\"],\n",
    "                           row[\"prediction_count\"], row[\"truth_count\"], count_ratio, row[\"area_score\"])\n",
    "\n",
    "        run.log({\"per_image_results\": table})\n",
    "\n",
    "        run.finish()\n",
    "        return df, avg_raad\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {e}\")\n",
    "        run.finish()\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b394b5ee-680b-4b34-8925-9ac4b32733e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kfold_yamls(split_dir: Path, k: int = 5):\n",
    "    assert split_dir.name.startswith(\"SID03\"), \"Only for SID03 Split.\"\n",
    "\n",
    "    train_img_dir = split_dir / \"images\" / \"train\"\n",
    "    all_imgs = list(train_img_dir.glob(\"*.jpg\"))\n",
    "    random.shuffle(all_imgs)\n",
    "\n",
    "    fold_size = len(all_imgs) // k\n",
    "    fold_root = split_dir.parent / f\"{split_dir.name}_kfold\"\n",
    "    fold_root.mkdir(exist_ok=True)\n",
    "\n",
    "    for fold in range(k):\n",
    "        fold_dir = fold_root / f\"fold{fold}\"\n",
    "\n",
    "        for phase in [\"train\", \"val\", \"test\"]:\n",
    "            (fold_dir / \"images\" / phase).mkdir(parents=True, exist_ok=True)\n",
    "            (fold_dir / \"labels\" / phase).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        val_set = set(all_imgs[fold * fold_size : (fold + 1) * fold_size])\n",
    "\n",
    "        for img_path in all_imgs:\n",
    "            phase = \"val\" if img_path in val_set else \"train\"\n",
    "            shutil.copy(img_path, fold_dir / \"images\" / phase / img_path.name)\n",
    "\n",
    "            label_found = False\n",
    "            for subtype in SUBTYPES:\n",
    "                lbl_src = split_dir / \"labels\" / \"train\" / subtype / f\"{img_path.stem}.txt\"\n",
    "                if lbl_src.exists():\n",
    "                    shutil.copy(lbl_src, fold_dir / \"labels\" / phase / lbl_src.name)\n",
    "                    label_found = True\n",
    "                    break\n",
    "            if not label_found:\n",
    "                print(f\"[!] No Labe data for: {img_path.name}\")\n",
    "\n",
    "        for phase in [\"images\", \"labels\"]:\n",
    "            src = split_dir / phase / \"test\"\n",
    "            dst = fold_dir / phase / \"test\"\n",
    "            if not dst.exists():\n",
    "                dst.symlink_to(src)\n",
    "\n",
    "        yaml_dict = {\n",
    "            \"train\": str((fold_dir / \"images/train\").resolve()),\n",
    "            \"val\": str((fold_dir / \"images/val\").resolve()),\n",
    "            \"test\": str((fold_dir / \"images/test\").resolve()),\n",
    "            \"nc\": 1,\n",
    "            \"names\": [\"lesion\"]\n",
    "        }\n",
    "        with open(fold_dir / \"data.yaml\", \"w\") as f:\n",
    "            yaml.safe_dump(yaml_dict, f)\n",
    "\n",
    "\n",
    "        csv_src = split_dir / \"labels\" / \"test\" / \"bboxes_test.csv\"\n",
    "        csv_dst = fold_dir / \"labels\" / \"test\" / \"bboxes_test.csv\"\n",
    "        if csv_src.exists():\n",
    "            shutil.copy(csv_src, csv_dst)\n",
    "        else:\n",
    "            print(f\"[Didnt found bboxes_test.csv: {csv_src}\")\n",
    "        \n",
    "        print(f\"Fold {fold} created: {fold_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354d3b3-4747-419e-bbb5-b2aec46b1d7a",
   "metadata": {},
   "source": [
    "# Start Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2788797-08c1-40fd-872f-d8a739c4afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_and_eval(\n",
    "    splits=[\"SID01\", \"SID02\"],            \n",
    "    models=[\"n\", \"m\", \"l\"],                \n",
    "    batches=[16, 32],                     \n",
    "    epochs=40,\n",
    "    lr=0.01,\n",
    "    train_project=\"maize_train\",\n",
    "    eval_project=\"maize_eval\",\n",
    "    sid02_subtypes=[None, \"drone\", \"boom\", \"handheld\"],  \n",
    "    sid03_folds=None,                     \n",
    "    device=\"0\"\n",
    "):\n",
    "    for split in splits:\n",
    "        if split == \"SID01\":\n",
    "            for model in models:\n",
    "                for batch in batches:\n",
    "                    run_name = f\"yolo11{model}_{split}_e{epochs}_b{batch}\"\n",
    "                    weights = train_model(\n",
    "                        model_size=model,\n",
    "                        split=split,\n",
    "                        epochs=epochs,\n",
    "                        batch=batch,\n",
    "                        lr=lr,\n",
    "                        project=train_project,\n",
    "                        run_name = run_name\n",
    "                    )\n",
    "                    evaluate_test_raad(\n",
    "                        model_path=weights,\n",
    "                        split=split,\n",
    "                        split_root=BASE_PATH / split,\n",
    "                        project =  eval_project,\n",
    "                        run_name=run_name\n",
    "                    )\n",
    "\n",
    "        elif split == \"SID02\":\n",
    "            for subtype in sid02_subtypes:\n",
    "                subtype_tag = subtype if subtype else \"all\"\n",
    "                subset_root = _make_sid02_subset(BASE_PATH / \"SID02\", subtype) if subtype else BASE_PATH / \"SID02\"\n",
    "\n",
    "                for model in models:\n",
    "                    for batch in batches:\n",
    "                        run_name = f\"yolo11{model}_{split}_{subtype}_e{epochs}_b{batch}\"\n",
    "                        weights = train_model(\n",
    "                            model_size=model,\n",
    "                            split=\"SID02\",\n",
    "                            subtype=subtype,\n",
    "                            epochs=epochs,\n",
    "                            batch=batch,\n",
    "                            lr=lr,\n",
    "                            project=train_project,\n",
    "                            run_name=run_name\n",
    "                        )\n",
    "                        evaluate_test_raad(\n",
    "                            model_path=weights,\n",
    "                            split=\"SID02\",\n",
    "                            split_root=subset_root,\n",
    "                            project =  eval_project,\n",
    "                            run_name=run_name\n",
    "                        )\n",
    "\n",
    "        elif split == \"SID03\":\n",
    "            sid03_kfold_root = BASE_PATH / \"SID03_kfold\"\n",
    "            if not sid03_kfold_root.exists():\n",
    "                print(\"SID03_kfold not found â€“ generate K-Folds...\")\n",
    "                generate_kfold_yamls(BASE_PATH / \"SID03\", k=5)\n",
    "        \n",
    "            folds = sid03_folds if sid03_folds else [None]\n",
    "\n",
    "            for fold in folds:\n",
    "                for model in models:\n",
    "                    for batch in batches:\n",
    "                        run_name = f\"yolo11{model}_{split}_f{fold}_e{epochs}_b{batch}\"\n",
    "                        weights = train_model(\n",
    "                            model_size=model,\n",
    "                            split=\"SID03\",\n",
    "                            fold_id=fold,\n",
    "                            epochs=epochs,\n",
    "                            batch=batch,\n",
    "                            lr=lr,\n",
    "                            project=train_project,\n",
    "                            run_name=run_name\n",
    "                        )\n",
    "                        fold_path = (BASE_PATH / \"SID03_kfold\" / f\"fold{fold}\") if fold is not None else BASE_PATH / \"SID03\"\n",
    "                        sid03_test_root = BASE_PATH / \"SID03\"\n",
    "                        evaluate_test_raad(\n",
    "                            model_path=weights,\n",
    "                            split=\"SID03\",\n",
    "                            split_root=sid03_test_root,\n",
    "                            project=eval_project,\n",
    "                            run_name=run_name\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfa73e-039e-4526-a87c-30b924caf5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.151 ðŸš€ Python-3.12.10 torch-2.7.1+cu126 CUDA:0 (NVIDIA A16, 14891MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/exchange/dspro2/M-AI-ZE/data/adjusted/1.2/splits/SID01/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo11s_SID01_e25_b8, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=V1_2-maize_disease_detection_train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=V1_2-maize_disease_detection_train/yolo11s_SID01_e25_b8, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "YOLO11s summary: 181 layers, 9,428,179 parameters, 9,428,163 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DSPRO2/M-AI-ZE-Maize-diseases-detection/notebooks/wandb/run-20250607_042249-3iiucgkt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rueedi-tobias-hochschule-luzern/V1_2-maize_disease_detection_train/runs/3iiucgkt' target=\"_blank\">yolo11s_SID01_e25_b8</a></strong> to <a href='https://wandb.ai/rueedi-tobias-hochschule-luzern/V1_2-maize_disease_detection_train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rueedi-tobias-hochschule-luzern/V1_2-maize_disease_detection_train' target=\"_blank\">https://wandb.ai/rueedi-tobias-hochschule-luzern/V1_2-maize_disease_detection_train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rueedi-tobias-hochschule-luzern/V1_2-maize_disease_detection_train/runs/3iiucgkt' target=\"_blank\">https://wandb.ai/rueedi-tobias-hochschule-luzern/V1_2-maize_disease_detection_train/runs/3iiucgkt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1903.0Â±607.2 MB/s, size: 53.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /exchange/dspro2/M-AI-ZE/data/adjusted/1.2/splits/SID01/labels/train.cache... 10866 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10866/10866 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1417.6Â±1051.0 MB/s, size: 56.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /exchange/dspro2/M-AI-ZE/data/adjusted/1.2/splits/SID01/labels/val.cache... 1358 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1358/1358 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to V1_2-maize_disease_detection_train/yolo11s_SID01_e25_b8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mV1_2-maize_disease_detection_train/yolo11s_SID01_e25_b8\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/25       2.2G       2.19       2.17      1.562         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1359/1359 [04:56<00:00,  4.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:15<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1358      10758      0.423      0.379      0.339      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/25      2.23G      2.077      1.861       1.49         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1359/1359 [04:47<00:00,  4.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:15<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1358      10758       0.48      0.404      0.389      0.165\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/25      2.23G      2.061      1.816      1.464         53        640:  10%|â–‰         | 133/1359 [00:27<04:15,  4.79it/s]"
     ]
    }
   ],
   "source": [
    "run_training_and_eval(\n",
    "    splits=[\"SID02\"],\n",
    "    models=[\"s\",\"n\",\"m\",\"l\"],\n",
    "    batches=[8, 16],\n",
    "    epochs=25,\n",
    "    train_project=TRAIN_PROJECT_PREFIX,\n",
    "    eval_project=EVAL_PROJECT_PREFIX,\n",
    "    sid02_subtypes=[\"boom\", \"drone\",\"handheld\"],\n",
    "    sid03_folds=[3, 4],\n",
    "    device=\"0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a418c-bf3c-469e-b891-2d17ffc24a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98e278-c747-422e-ac8a-d79c4d25a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
